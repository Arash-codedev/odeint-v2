[/============================================================================
  Boost.odeint

  Copyright (c) 2009-2012 Karsten Ahnert
  Copyright (c) 2009-2012 Mario Mulansky

  Use, modification and distribution is subject to the Boost Software License,
  Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at
  http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]


[section Parallel computation with OpenMP and MPI]

Paralleliaztion is a key feature for modern numerical libraries due to the
vast availability of many cores nowadays even on Laptops.
odeint currently supports parallelization with OpenMP and MPI, as described in
the following sections.
However, it should be made clear from the beginning that the difficulty of
efficiently distributing ODE integration on many cores/machines lies in the
parallelization of the rhs function, which is still the user's
responsibility.
Simply using parallel odeint backend without parallelizing the rhs function
will bring you almost no performance gains.

[section OpenMP]

[import ../examples/openmp/phase_chain.cpp]

odeint's OpenMP support is implemented as an external backend, which needs to be manually included. Depending on the compiler some additional flags may be needed, i.e. `-fopenmp` for GCC.
[phase_chain_openmp_header]

In the easiest parallelization approach with OpenMP we use a usual `vector` as
state type:
```typedef std::vector< double > state_type;```
We initialize the state with some random data:
[phase_chain_init]
Now we have to configure the stepper to use the OpenMP backend.
This is done by explicitely providing the `openmp_range_algebra` as template parameter
to the stepper.
This algebra requires the state type to be a model of Random Access Range.
[phase_chain_stepper]

Additionally to providing the stepper with OpenMP parallelization we also have
to provide a parallelized rhs function.
Here this is shown for a simple one-dimensional chain of phase oscillators
with nearest neighbor coupling:
[phase_chain_rhs]

Finally, we perform the integration by using an integrate function from
odeint.
As you can see, the parallelization is completely hidden in the stepper and
the rhs function.
OpenMP will take care of distributing the work among the threads and join them automatically.
[phase_chain_integrate]

OpenMP took care of all the parallelization details and the data can be
accessed immediately and be processed further.
Note, that you can specify the OpenMP scheduling by calling `omp_set_schedule`
in the beginning of your program:
[phase_chain_scheduling]

[import ../examples/openmp/phase_chain_omp_state.cpp]

For advanced cases odeint offers another approach to use OpenMP 
that allows for a more exact control of the parallelization.
For example, for odd-sized data where OpenMP's thread boundaries don't match
cache lines and hurt performance it might be advisable to copy the data from
the continuous `vector<T>` into separate, individually aligned, vectors. 
For this, odeint provides the `openmp_state<T>` type, essentially an alias for
`vector<vector<T>>`.

Here, the initialization is done using a usual `vector<double>`, but then we
use odeint's `split` function to generate an `openmp_state`.
The splitting is done such the sizes of the individual regions differ at most
by 1 to make the computation as uniform as possible.

[phase_chain_state_init]

Of course, also the rhs function has to be changed so that it can deal with
the `openmp_state`.
Note, that each sub-regions of the state is computed in a single task, but at
the borders access to the neighboring regions is required.

[phase_chain_state_rhs]

Using the `openmp_state<T>` state type automatically selects `openmp_algebra`
which executes odeint's internal computations on parallel chunks.
Hence, no manual configuration of the stepper is neccessary.
At the end of the integration, we use `unsplit` to merge the sub-region back
together into a single vector.
[phase_chain_state_integrate]

[note
You don't actually need to use `openmp_state<T>` for advanced use cases, `openmp_algebra` is simply an alias for `openmp_nested_algebra<range_algebra>` and supports any model of Random Access Range as the outer, parallel state type, and will use the given algebra on its elements.]

[endsect]

[section MPI]

[import ../performance/mpi/osc_chain_1d.cpp]
[import ../performance/mpi/osc_chain_1d_system.hpp]

To expand the parallel computation across multiple machines we can use MPI.

The `osc_chain::operator()` implementation is similar to the OpenMP variant with split data, the main difference being that while OpenMP uses a spawn/join model where everything not explicitly parallelised is only executed in the main thread, in MPI's prefork model each node enters the `main()` method independently, diverging based on its rank and synchronising through message-passing and explicit barriers.

Instead of reading another thread's data, we asynchronously send and receive the relevant data from neighboring nodes, performing some computation in the interim.
[parallel_chain_mpi_system]

odeint's MPI support is implemented as an external backend, too. Depending on the MPI implementation the code might need to be compiled with i.e. `mpic++`.
[parallel_chain_mpi_header]

In the main program we construct a `communicator` which tells us the current node's `rank()` and total `size()` of the cluster.
[parallel_chain_mpi_init]

We generate the input data on the master node only, avoiding unnecessary work on the other nodes.
[parallel_chain_mpi_data]

Analogous to `openmp_state<T>` we use `mpi_state<InnerState<T>>`, which automatically selects `mpi_nested_algebra` and the appropriate MPI-oblivious inner algebra.
Instead of simply copying chunks, `split` here sends the data from master to each slave and receives data from master on slaves. The input argument is ignored on the slaves, but the master node receives a chunk of data in its output and will participate in the communication.
[parallel_chain_mpi_state]

Now that `q_split` contains (only) the local chunk for each node, we enter the computation. Be aware that `mpi_nested_algebra::for_each`[~N] doesn't use any MPI constructs, it simply calls the inner algebra on the local chunk, the system function is not guarded by any barriers either, if you don't manually place any (for example in parameter studies cases where the elements are completely independent) you might see the nodes diverging, returning from this call at different times.
[parallel_chain_mpi_run]

To print the result on the master node, we send the processed data back using `unsplit`.
[parallel_chain_mpi_fin]


[endsect]

[section Concepts]

[section MPI State]
As used by `mpi_nested_algebra`.
[heading Notation]
[variablelist
    [[`InnerState`] [The inner state type]]
    [[`State`] [The MPI-state type]]
    [[`state`] [Object of type `State`]]
    [[`world`] [Object of type `boost::mpi::communicator`]]
]
[heading Valid Expressions]
[table
    [[Name] [Expression] [Type] [Semantics]]
    [[Construct a state with a communicator] [`State(world)`] [`State`] [Constructs the State.]]
    [[Construct a state with the default communicator] [`State()`] [`State`] [Constructs the State.]]
    [[Get the current node's inner state] [`state()`] [`InnerState`] [Returns a (const) reference.]]
    [[Get the communicator] [`state.world`] [`boost::mpi::communicator`] [See __boost_mpi.]]
]
[heading Models]
* `mpi_state<InnerState>`

[endsect]

[section OpenMP Split State]
As used by `openmp_nested_algebra`, essentially a Random Access Container with `ValueType = InnerState`.
[heading Notation]
[variablelist
    [[`InnerState`] [The inner state type]]
    [[`State`] [The split state type]]
    [[`state`] [Object of type `State`]]
]
[heading Valid Expressions]
[table
    [[Name] [Expression] [Type] [Semantics]]
    [[Construct a state for `n` chunks] [`State(n)`] [`State`] [Constructs underlying `vector`.]]
    [[Get a chunk] [`state[i]`] [`InnerState`] [Accesses underlying `vector`.]]
    [[Get the number of chunks] [`state.size()`] [`size_type`] [Returns size of underlying `vector`.]]
]
[heading Models]
* `openmp_state<ValueType>` with `InnerState = vector<ValueType>`

[endsect]

[section Splitter]
[heading Notation]
[variablelist
    [[`Container1`] [The continuous-data container type]]
    [[`x`] [Object of type `Container1`]]
    [[`Container2`] [The chunked-data container type]]
    [[`y`] [Object of type `Container2`]]
]
[heading Valid Expressions]
[table
    [[Name] [Expression] [Type] [Semantics]]
    [[Copy chunks of input to output elements] [`split(x, y)`] [`void`] [Calls `split_impl<Container1, Container2>::split(x, y)`, splits `x` into `y.size()` chunks.]]
    [[Join chunks of input elements to output] [`unsplit(y, x)`] [`void`] [Calls `unsplit_impl<Container2, Container1>::unsplit(y, x)`, assumes `x` is of the correct size ['__sigma `y[i].size()`], does not resize `x`.]]
]
[heading Models]
* defined for `Container1` = __boost_range and `Container2 = openmp_state`
* and `Container2 = mpi_state`.

To implement splitters for non-__boost_range-based containers, specialise the `split_impl` and `unsplit_impl` types:
```
template< class Container1, class Container2 , class Enabler = void >
struct split_impl {
    static void split( const Container1 &from , Container2 &to );
};

template< class Container2, class Container1 , class Enabler = void >
struct unsplit_impl {
    static void unsplit( const Container2 &from , Container1 &to );
};
```
[endsect]

[endsect]

[endsect]
